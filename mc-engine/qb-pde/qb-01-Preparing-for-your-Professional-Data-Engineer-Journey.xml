<?xml version='1.0' encoding='UTF-8'?>
<bank>
  <topic>PDE01 - Preparing for your Professional Data Engineer Journey</topic>
  <entry>
    <question>You are migrating on-premises data to a data warehouse on Google Cloud. This data will be made available to business analysts. Local regulations require that customer information including credit card numbers, phone numbers, and email IDs be captured, but not used in analysis. You need to use a reliable, recommended solution to redact the sensitive data. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a regular expression to identify and delete patterns that resemble credit card numbers, phone numbers, and email IDs.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Delete all columns with a title similar to "credit card," "phone," and "email."</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Use the Cloud Data Loss Prevention (DLP) API to identify and redact data that matches infoTypes like credit card numbers, phone numbers, and email IDs.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the Cloud Data Loss Prevention (DLP) API to perform date shifting of any entries with credit card numbers, phone numbers, and email IDs.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Business analysts in your team need to run analysis on data that was loaded into BigQuery. You need to follow recommended practices and grant permissions. What role should you grant the business analysts?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>bigquery.dataOwner</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>bigquery.user and bigquery.dataViewer</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>bigquery.resourceViewer and bigquery.dataViewer</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>storage.objectViewer and bigquery.user</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Cymbal Retail has acquired another company in Europe. Data access permissions and policies in this new region differ from those in Cymbal Retail’s headquarters, which is in North America. You need to define a consistent set of policies for projects in each region that follow recommended practices. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Implement policies at the resource level that comply with regional laws.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Implement a flat hierarchy, and assign policies to each project according to its region.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a new organization for all projects in Europe and assign policies in each organization that comply with regional laws.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Create top level folders for each region, and assign policies at the folder level.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You are managing the data for Cymbal Retail, which consists of multiple teams including retail, sales, marketing, and legal. These teams are consuming data from multiple producers including point of sales systems, industry data, orders, and more. Currently, teams that consume data have to repeatedly ask the teams that produce it to verify the most up-to-date data and to clarify other questions about the data, such as source and ownership. This process is unreliable and time-consuming and often leads to repeated escalations. You need to implement a centralized solution that gains a unified view of the organization's data and improves searchability. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Implement a data lake with Cloud Storage, and create buckets for each team such as retail, sales, marketing.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Implement a data mesh with Dataplex and have producers tag data when created.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Implement a data warehouse by using BigQuery, and create datasets for each team such as retail, sales, marketing.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Implement Looker dashboards that provide views of the data that meet each teams’ requirements.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You have a Dataflow pipeline that runs data processing jobs. You need to identify the parts of the pipeline code that consume the most resources. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Audit Logs</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Logging</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Use Cloud Profiler</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Monitoring</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You are using Dataproc to process a large number of CSV files. The storage option you choose needs to be flexible to serve many worker nodes in multiple clusters. These worker nodes will read the data and also write to it for intermediate storage between processing jobs. What is the recommended storage option on Google Cloud?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Local SSD</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Zonal persistent disks</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Cloud Storage</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud SQL</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Your data and applications reside in multiple geographies on Google Cloud. Some regional laws require you to hold your own keys outside of the cloud provider environment, whereas other laws are less restrictive and allow storing keys with the same provider who stores the data. The management of these keys has increased in complexity, and you need a solution that can centrally manage all your keys. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Store keys in Cloud Key Management Service (KMS), and reduce the number of days for automatic key rotation.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Enable confidential computing for all your virtual machines.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Store your keys on a supported external key management partner, and use Cloud External Key Manager (EKM) to get keys when required.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store your keys in Cloud Hardware Security Module (HSM), and retrieve keys from it when required.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Cymbal Retail is migrating its private data centers to Google Cloud. Over many years, hundreds of terabytes of data were accumulated. You currently have a 100 Mbps line and you need to transfer this data reliably before commencing operations on Google Cloud in 45 days. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Zip and upload the data to Cloud Storage buckets by using the Google Cloud console.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Upload the data to Cloud Storage by using gsutil.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store the data in an HTTPS endpoint, and configure Storage Transfer Service to copy the data to Cloud Storage.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Order a transfer appliance, export the data to it, and ship it to Google.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Laws in the region where you operate require that files related to all orders made each day are stored immutably for 365 days. The solution that you recommend has to be cost-effective. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Store the data in a Cloud Storage bucket, enable object versioning, and delete any version greater than 365.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store the data in a Cloud Storage bucket, and set a lifecycle policy to delete the file after 365 days.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Store the data in a Cloud Storage bucket, and specify a retention period.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store the data in a Cloud Storage bucket, and enable object versioning and delete any version older than 365 days.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Cymbal Retail has a team of business analysts who need to fix and enhance a set of large input data files. For example, duplicates need to be removed, erroneous rows should be deleted, and missing data should be added. These steps need to be performed on all the present set of files and any files received in the future in a repeatable, automated process. The business analysts are not adept at programming. What should they do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a Dataflow pipeline with the data fixes you need.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Load the data into Google Sheets, explore the data, and fix the data as needed.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a Dataproc job to perform the data fixes you need.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Load the data into Dataprep, explore the data, and edit the transformations as needed.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You need to run batch jobs, which could take many days to complete. You do not want to manage the infrastructure provisioning. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Run to run the jobs.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Run the jobs on Batch.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Workflows to run the jobs.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Scheduler to run the jobs.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You manage a PySpark batch data pipeline by using DataproC: You want to take a hands-off approach to running the workload, and you do not want to provision and manage your own cluster. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Configure the job to run with Spot VMs.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Rewrite the job in Dataflow with SQL.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Configure the job to run on Dataproc Serverless.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Rewrite the job in Spark SQL.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>The first stage of your data pipeline processes tens of terabytes of financial data and creates a sparse, time-series dataset as a key-value pair. Which of these is a suitable sink for the pipeline's first stage?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Bigtable</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>AlloyDB</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud SQL</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Storage</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Your data engineering team receives data in JSON format from external sources at the end of each day. You need to design the data pipeline. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Make your BigQuery data warehouse public and ask the external sources to insert the data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store the data in persistent disks and create an ETL pipeline.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Store the data in Cloud Storage and create an extract, transform, and load (ETL) pipeline.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a public API to allow external applications to add the data to your warehouse.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You have a data pipeline that requires you to monitor a Cloud Storage bucket for a file, start a Dataflow job to process data in the file, run a shell script to validate the processed data in BigQuery, and then delete the original file. You need to orchestrate this pipeline by using recommended tools. Which product should you choose?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Cloud Composer</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Run</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Tasks</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Scheduler</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Your company has multiple data analysts but a limited data engineering team. You need to choose a tool where the analysts can build data pipelines themselves with a graphical user interface. Which of these products is the most appropriate?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Cloud Data Fusion</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Dataflow</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Dataproc</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Composer</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You are processing large amounts of input data in BigQuery. You need to combine this data with a small amount of frequently changing data that is available in Cloud SQL. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a Dataflow pipeline to combine the BigQuery and Cloud SQL data when the Cloud SQL data changes.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Copy the data from Cloud SQL to a new BigQuery table hourly.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Copy the data from Cloud SQL and create a combined, normalized table hourly.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Use a federated query to get data from Cloud SQL.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You are creating a data pipeline for streaming data on Dataflow for Cymbal Retail's point of sales data. You want to calculate the total sales per hour on a continuous basis. Which of these windowing options should you use?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Tumbling windows (fixed windows in Apache Beam)</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Hopping windows (sliding windows in Apache Beam)</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Session windows</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Global window</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You are running Dataflow jobs for data processing. When developers update the code in Cloud Source Repositories, you need to test and deploy the updated code with minimal effort. Which of these would you use to build your continuous integration and delivery (CI/CD) pipeline for data processing?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Cloud Build</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Code</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Compute Engine</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Terraform</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You want to build a streaming data analytics pipeline in Google Cloud. You need to choose the right products that support streaming data. Which of these would you choose?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Pub/Sub, Dataprep, BigQuery</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Pub/Sub, Dataflow, BigQuery</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Storage, Dataflow, Cloud SQL</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Storage, Dataprep, AlloyDB</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>A company collects large amounts of data that is useful for improving business operations. The collected data is already clean and is in a format that is suitable for further analysis. The company uses Google Cloud BigQuery as a data warehouse. What approach will you recommend to move this data to BigQuery?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Directly load the data using Extract and Load approach (EL).</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Split the data into smaller files and then move to Google Cloud.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Implement Extract, Transform and Load (ETL) pipelines using tools like Dataflow.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Do transformation using Extract, Load and Transform (ELT).</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>A company wants to improve productivity and decides to programmatically schedule and monitor workflows. What tool can you use to automate your workflows?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Data Fusion</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Apache Beam and Dataflow</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Cloud Composer</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Dataproc</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Cymbal Retail collects large amounts of data that is useful for improving business operations. The company wants to store and analyze this data in a serverless and cost-effective manner using Google Cloud. The analysts need to use SQL to write the queries. What tool can you use to meet these requirements?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>BigQuery</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Data Fusion</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Spanner</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Memorystore</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Cymbal Retail also collects large amounts of structured, semistructured, and unstructured data. The company wants a centralized repository to store this data in a cost-effective manner using Google Cloud. What tool can you use to meet these requirements?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Cloud Storage</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud SQL</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Dataflow</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Bigtable</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Your company uses Google Workspace and your leadership team is familiar with its business apps and collaboration tools. They want a cost-effective solution that uses their existing knowledge to evaluate, analyze, filter, and visualize data that is stored in BigQuery. What should you do to create a solution for the leadership team?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Configure Looker Studio.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Configure Connected Sheets.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Configure Tableau.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create models in Looker.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Your data in BigQuery has some columns that are extremely sensitive. You need to enable only some users to see certain columns. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Use policy tags.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a new table with the column's data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a new dataset with the column's data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Identity and Access Management (IAM) permissions.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You have a complex set of data that comes from multiple sources. The analysts in your team need to analyze the data, visualize it, and publish reports to internal and external stakeholders. You need to make it easier for the analysts to work with the data by abstracting the multiple data sources. What tool do you recommend?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Connected Sheets</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Looker</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Looker Studio</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>D3.js library</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Your business has collected industry-relevant data over many years. The processed data is useful for your partners and they are willing to pay for its usage. You need to ensure proper access control over the data. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Host the data on Cloud SQL.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Host the data on Analytics Hub.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Export the data to persistent disks and share it through an FTP endpoint.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Export the data to zip files and share it through Cloud Storage.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You have data in PostgreSQL that was designed to reduce redundancy. You are transferring this data to BigQuery for analytics. The source data is hierarchical and frequently queried together. You need to design a BigQuery schema that is performant. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Copy the normalized data into partitions.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Retain the data in normalized form always.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Copy the primary tables and use federated queries for secondary tables.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Use nested and repeated fields.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You repeatedly run the same queries by joining multiple tables. The original tables change about ten times per day. You want an optimized querying approach. Which feature should you use?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Federated queries</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Materialized views</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Views</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Partitions</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You built machine learning (ML) models based on your own data. In production, the ML models are not giving satisfactory results. When you examine the data, it appears that the existing data is not sufficiently representing the business goals. You need to create a more accurate machine learning model. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Perform L2 regularization.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Train the model with the same data, but use more epochs.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Perform feature engineering, and use domain knowledge to enhance the column data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Train the model with more of similar data.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You used Dataplex to create lakes and zones for your business data. However, some files are not being discovered. What could be the issue?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>You have scheduled discovery to run every hour.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The files are in Parquet format.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>You have an exclude pattern that matches the files.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The files are in ORC format.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You have analytics data stored in BigQuery. You need an efficient way to compute values across a group of rows and return a single result for each row. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Use a window function with an OVER clause.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use a UDF (user-defined function).</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use an aggregate function.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use BigQuery ML.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You need to optimize the performance of queries in BigQuery. Your tables are not partitioned or clustered. What optimization technique can you use?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Batch your updates and inserts.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Filter data as late as possible.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Perform self-joins on data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use the LIMIT clause to reduce the data read.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Cymbal Retail has a team of ML engineers that builds and maintains machine learning models. As a Professional Data Engineer, how will you support this team?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Finalize the type of machine learning model to use.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Identify what type of data is required to build ML models.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Keep on improving the machine learning model after initial deployment.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Process and prepare existing data to enable feature engineering.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You need to share inventory data from Cymbal Retail with a partner company that uses BigQuery to store and analyze its data. What tool can you use to securely and efficiently share the data?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Cloud Storage</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Data Catalog</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Analytics Hub</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Data Loss Prevention (DLP)</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>A colleague at Cymbal Retail asks you about the configuration of Dataproc autoscaling for a project. What would be the Google-recommended situation when you should enable autoscaling?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>When you want to down-scale idle clusters to minimum size.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>When you want to scale on-cluster Hadoop Distributed File System (HDFS).</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>When there are different size workloads on the cluster.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>When you want to scale out single-job clusters.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>When running Dataflow jobs, you see this error in the logs: "A hot key HOT_KEY_NAME was detected in…". You need to resolve this issue and make the workload performant. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Add more compute instances for processing.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Increase the data with the hot key.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Disable Dataflow shuffle.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Ensure that your data is evenly distributed.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Multiple analysts need to prepare reports on Monday mornings due to which there is heavy utilization of BigQuery. You want to take a cost-effective approach to managing this demand. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use BigQuery Enterprise Plus edition with a three-year commitment.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use on-demand pricing.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Use Flex Slots.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use BigQuery Enterprise edition with a one-year commitment.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You need to create repeatable data processing tasks by using Cloud Composer. You need to follow best practices and recommended approaches. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Use current time with the now() function for computation.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Write each task to be responsible for one operation.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Update data with INSERT statements during the task run.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Combine multiple functionalities in a single task execution.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You need to design a Dataproc cluster to run multiple small jobs. Many jobs (but not all) are of high priority. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Reuse the same cluster to run all jobs in parallel.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use cluster autoscaling.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Reuse the same cluster and run each job in sequence.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Use ephemeral clusters.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You have a team of data analysts that run queries interactively on BigQuery during work hours. You also have thousands of report generation queries that run simultaneously. You often see an error: Exceeded rate limits: too many concurrent queries for this project_and_region. How would you resolve this issue?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a view to run the queries.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Run the report generation queries in batch mode.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a yearly reservation of BigQuery slots.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Run all queries in interactive mode.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You run a Cloud SQL instance for a business that requires that the database is accessible for transactions. You need to ensure minimal downtime for database transactions. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Configure high availability.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Configure backups and increase the number of backups.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Configure replication.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Configure backups.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You are running a Dataflow pipeline in production. The input data for this pipeline is occasionally inconsistent. Separately from processing the valid data, you want to efficiently capture the erroneous input data for analysis. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Check for the erroneous data in the logs.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Read the data once, and split it into two pipelines, one to output valid data and another to output erroneous data.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Create a side output for the erroneous data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Re-read the input data and create separate outputs for valid and erroneous data.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You have a Dataflow pipeline in production. For certain data, the system seems to be stuck longer than usual. This is causing delays in the pipeline execution. You want to reliably and proactively track and resolve such issues. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Set up alerts with Cloud Functions code that reviews the audit logs regularly.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Review the Dataflow logs regularly.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Set up alerts on Cloud Monitoring based on system lag.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Review the Cloud Monitoring dashboard regularly.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Cymbal Retail processes streaming data on Dataflow with Pub/Sub as a source. You need to plan for disaster recovery and protect against zonal failures. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Enable vertical autoscaling.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create Dataflow jobs from templates.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Enable Dataflow shuffle.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Take Dataflow snapshots periodically.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Cymbal Retail uses Google Cloud and has automated repeatable data processing workloads to achieve reliability and cost efficiency. You want out-of-the-box metric collection dashboards and the ability to generate alerts when specific conditions are met. What tool can you use?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Data Catalog</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Cloud Monitoring</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Cloud Composer</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Data Loss Prevention (DLP)</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Your company recently migrated to Google Cloud and started using BigQuery. The team members don’t know how much querying they are going to do, and they need to be efficient with their spend. As a Professional Data Engineer, what pricing model would you recommend?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Decide how much compute capacity you need and reserve it using capacity pricing.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Use BigQuery’s on-demand pricing model.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use IAM service to block access to BigQuery till the team figures out how much querying they are going to do.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a pool of resources using BigQuery Reservations.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Your company is very serious about data protection and hence decides to implement the Principle of Least Privilege. What should you do to comply with this policy?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>When a task is assigned, ensure that it gets assigned to a person with the minimum privileges.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Give just enough permissions to get the task done.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Ensure that the users are verified every time they request access, even if they were authenticated earlier.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Ensure that the access permissions are given strictly based on the person’s title and job role.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>A company collects lots of consumer data from online marketing campaigns. Company plans to use Google Cloud to store this collected data. The top management is worried about exposing personally identifiable information (PII) that may be present in this data. What should you do to reduce the risk of exposing PII data?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Ensure that all PII data is removed from the collected data before storing it on Google Cloud.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Store all data in BigQuery and turn on column level access to protect sensitive data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Ensure that all stored data is monitored by Security Command Center.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Use Cloud Data Loss Prevention (Cloud DLP) to inspect and redact PII data.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You are ingesting data that is spread out over a wide range of dates into BigQuery at a fast rate. You need to partition the table to make queries performant. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Create an ingestion-time partitioned table with daily partitioning type.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create an integer-range partitioned table.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create an ingestion-time partitioned table with yearly partitioning type.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a time-unit column-partitioned table with yearly partitioning type.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You have several large tables in your transaction databases. You need to move all the data to BigQuery for the business analysts to explore and analyze the data. How should you design the schema in BigQuery?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Retain the data on BigQuery with the same schema as the source.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Combine all the transactional database tables into a single table using outer joins.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Redesign the schema to denormalize the data with nested and repeated data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Redesign the schema to normalize the data by removing all redundancies.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You need to store data long term and use it to create quarterly reports. What storage class should you choose?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Standard</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Archive</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Nearline</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Coldline</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You have data stored in a Cloud Storage bucket. You are using both Identity and Access Management (IAM) and Access Control Lists (ACLs) to configure access control. Which statement describes a user's access to objects in the bucket?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>The user has no access if either IAM or ACLs deny a permission.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The user has no access if IAM denies the permission.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>The user has access if either IAM or ACLs grant a permission.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>The user only has access if both IAM and ACLs grant a permission.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You have large amounts of data stored on Cloud Storage and BigQuery. Some of it is processed, but some is yet unprocessed. You have a data mesh created in Dataplex. You need to make it convenient for internal users of the data to discover and use the data. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Create a raw zone for the unprocessed data and a curated zone for the processed data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a lake for BigQuery data and a zone for Cloud Storage data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a lake for unprocessed data and assets for processed data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a lake for Cloud Storage data and a zone for BigQuery data.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You need to choose a data storage solution to support a transactional system. Your customers are primarily based in one region. You want to reduce your administration tasks and focus engineering effort on building your business application. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Use Cloud SQL.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Install a database of your choice on a Compute Engine VM.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a Cloud Storage bucket with a regional bucket.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Use Cloud Spanner.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>You have data that is ingested daily and frequently analyzed in the first month. Thereafter, the data is retained only for audits, which happen occasionally every few years. You need to configure cost-effective storage. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a bucket on Cloud Storage with object versioning configured.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Create a bucket on Cloud Storage with Autoclass configured.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Configure a lifecycle policy on Cloud Storage.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Configure a data retention policy on Cloud Storage.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Cymbal Retail has accumulated a large amount of data. Analysts and leadership are finding it difficult to understand the meaning of the data, such as BigQuery columns. Users of the data don't know who owns what. You need to improve the searchability of the data. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>true</valid>
          <detail>Create tags for data entries in Cloud Catalog.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Rename BigQuery columns with more descriptive names.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Add a description column corresponding to each data column.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Export the data to Cloud Storage with descriptive file names.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>Your analysts repeatedly run the same complex queries that combine and filter through a lot of data on BigQuery. The data changes frequently. You need to reduce the effort for the analysts. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Create a dataset with the data that is frequently queried.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Export the frequently queried data into a new table.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Create a view of the frequently queried data.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Export the frequently queried data into Cloud SQL.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <entry>
    <question>A manager at Cymbal Retail expresses concern about unauthorized access to objects in your Cloud Storage bucket. You need to evaluate all access on all objects in the bucket. What should you do?</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>Review the Admin Activity audit logs.</detail>
        </option>
        <option>
          <valid>true</valid>
          <detail>Enable and then review the Data Access audit logs.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Change the permissions on the bucket to only trusted employees.</detail>
        </option>
        <option>
          <valid>false</valid>
          <detail>Route the Admin Activity logs to a BigQuery sink and analyze the logs with SQL queries.</detail>
        </option>
      </options>
    </answer>
  </entry>
  <!--
  STRUCTURE DEFINITION:
  <entry>
    <question>XXX</question>
    <answer>
      <options>
        <option>
          <valid>false</valid>
          <detail>XXX</detail>
        </option>
      </options>
    </answer>
  </entry>
  -->
</bank>